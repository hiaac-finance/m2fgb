{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from fairgbm import FairGBMClassifier\n",
    "\n",
    "\n",
    "sys.path.append('../scripts')\n",
    "import data\n",
    "import models\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(\n",
    "        trial,\n",
    "        scorer,\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        A_train,\n",
    "        X_val,\n",
    "        Y_val,\n",
    "        A_val,\n",
    "        model_class,\n",
    "        param_space,\n",
    "        random_state = None\n",
    "):  \n",
    "    params = {}\n",
    "    for name, values in param_space.items():\n",
    "        if values[\"type\"] == \"int\":\n",
    "            values_cp = {n: v for n, v in values.items() if n != \"type\"}\n",
    "            params[name] = trial.suggest_int(name, **values_cp)\n",
    "        elif values[\"type\"] == \"categorical\":\n",
    "            values_cp = {n: v for n, v in values.items() if n != \"type\"}\n",
    "            params[name] = trial.suggest_categorical(name, **values_cp)\n",
    "        elif values[\"type\"] == \"float\":  # corrected this line\n",
    "            values_cp = {n: v for n, v in values.items() if n != \"type\"}\n",
    "            params[name] = trial.suggest_float(name, **values_cp)\n",
    "\n",
    "    model = model_class(**params)\n",
    "    if isinstance(model, FairGBMClassifier):\n",
    "        model.fit(X_train, Y_train, constraint_group=A_train)\n",
    "    else:\n",
    "        model.fit(X_train, Y_train, A_train)\n",
    "    Y_val_pred = model.predict(X_val)\n",
    "    return scorer(Y_val, Y_val_pred, A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_feature(dataset, X_train, X_val, X_test):\n",
    "    if dataset == \"german2\":\n",
    "        A_train = X_train.Gender.astype(str)\n",
    "        A_val = X_val.Gender.astype(str) \n",
    "        A_test = X_test.Gender.astype(str)\n",
    "    elif dataset == \"adult\":\n",
    "        A_train = X_train.sex.astype(str)\n",
    "        A_val = X_val.sex.astype(str)\n",
    "        A_test = X_test.sex.astype(str)\n",
    "\n",
    "    sensitive_map = dict([\n",
    "        (attr, i)\n",
    "        for i, attr in enumerate(A_train.unique())\n",
    "    ])\n",
    "    A_train = A_train.map(sensitive_map)\n",
    "    A_val = A_val.map(sensitive_map)\n",
    "    A_test = A_test.map(sensitive_map)\n",
    "    return A_train, A_val, A_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y_ground, y_prob, y_pred, A):\n",
    "    acc = accuracy_score(y_ground, y_pred)\n",
    "    roc = roc_auc_score(y_ground, y_prob)\n",
    "    eq_loss = utils.equalized_loss_score(y_ground, y_prob, A)\n",
    "    eod = utils.equal_opportunity_score(y_ground, y_pred, A)\n",
    "    spd = utils.statistical_parity_score(y_ground, y_pred, A)\n",
    "    return {\n",
    "        \"acc\" : acc,\n",
    "        \"roc\" : roc,\n",
    "        \"eq_loss\" : eq_loss,\n",
    "        \"eod\" : eod,\n",
    "        \"spd\" : spd\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, random_state=None):\n",
    "    if model_name == \"XtremeFair\":\n",
    "        def model(**params):\n",
    "            return models.XtremeFair(random_state= random_state, **params)\n",
    "    elif model_name == \"XtremeFair_grad\":\n",
    "        def model(**params):\n",
    "            return models.XtremeFair(dual_learning=\"gradient\", random_state= random_state, **params)\n",
    "    elif model_name == \"XGBClassifier\":\n",
    "        def model(**params):\n",
    "            assert params[\"fair_weight\"] == 0\n",
    "            return models.XtremeFair(random_state= random_state, **params)\n",
    "    elif model_name == \"FairGBMClassifier\":\n",
    "        def model(**params):\n",
    "            return FairGBMClassifier(random_state= random_state, **params)\n",
    "    return model\n",
    "    \n",
    "def get_param_spaces(model_name):\n",
    "    if model_name == \"XtremeFair\":\n",
    "        return models.PARAM_SPACES[\"XtremeFair\"]\n",
    "    elif model_name == \"XtremeFair_grad\":\n",
    "        return models.PARAM_SPACES[\"XtremeFair\"]\n",
    "    elif model_name == \"XGBClassifier\":\n",
    "        return models.PARAM_SPACES[\"XGBClassifier\"]\n",
    "    elif model_name == \"FairGBMClassifier\":\n",
    "        return models.PARAM_SPACES[\"FairGBMClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_experiment(args):\n",
    "    # create output directory if not exists\n",
    "    if not os.path.exists(args[\"output_dir\"]):\n",
    "        os.makedirs(args[\"output_dir\"])\n",
    "\n",
    "    # clear best_params.txt if exists\n",
    "    if os.path.exists(os.path.join(args[\"output_dir\"], f\"best_params.txt\")):\n",
    "        os.remove(os.path.join(args[\"output_dir\"], f\"best_params.txt\"))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    cat_features = data.CAT_FEATURES[args[\"dataset\"]]\n",
    "    num_features = data.NUM_FEATURES[args[\"dataset\"]]\n",
    "    col_trans = ColumnTransformer(\n",
    "        [\n",
    "            (\"numeric\", StandardScaler(), num_features),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                OneHotEncoder(\n",
    "                    drop=\"if_binary\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                ),\n",
    "                cat_features,\n",
    "            ),\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    col_trans.set_output(transform=\"pandas\")\n",
    "    scorer = utils.get_combined_metrics_scorer(\n",
    "        alpha=args[\"alpha\"], performance_metric=\"acc\", fairness_metric=\"eod\"\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        # Load and prepare data\n",
    "        X_train, Y_train, X_val, Y_val, X_test, Y_test = data.get_fold(\n",
    "            args[\"dataset\"], i, 0\n",
    "        )\n",
    "\n",
    "        # Define sensitive attribute from gender and age\n",
    "        A_train, A_val, A_test = get_group_feature(\n",
    "            args[\"dataset\"], X_train, X_val, X_test\n",
    "        )\n",
    "\n",
    "        preprocess = Pipeline([(\"preprocess\", col_trans)])\n",
    "        preprocess.fit(X_train)\n",
    "        X_train = preprocess.transform(X_train)\n",
    "        X_val = preprocess.transform(X_val)\n",
    "        X_test = preprocess.transform(X_test)\n",
    "\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        objective = lambda trial: run_trial(\n",
    "            trial,\n",
    "            scorer,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            A_train,\n",
    "            X_val,\n",
    "            Y_val,\n",
    "            A_val,\n",
    "            get_model(args[\"model_name\"]),\n",
    "            get_param_spaces(args[\"model_name\"]),\n",
    "            0,\n",
    "        )\n",
    "        study.optimize(objective, n_trials=args[\"n_trials\"])\n",
    "\n",
    "        # save best params\n",
    "        with open(os.path.join(args[\"output_dir\"], f\"best_params.txt\"), \"a+\") as f:\n",
    "            f.write(str(study.best_params))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        model = get_model(args[\"model_name\"])(**study.best_params)\n",
    "        if isinstance(model, FairGBMClassifier):\n",
    "            model.fit(X_train, Y_train, constraint_group=A_train)\n",
    "        else:\n",
    "            model.fit(X_train, Y_train, A_train)\n",
    "        y_prob = model.predict_proba(X_train)[:, 1]\n",
    "        thresh = utils.get_best_threshold(Y_train, y_prob)\n",
    "        y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred_test = y_prob_test > thresh\n",
    "\n",
    "        metrics = eval_model(Y_test, y_prob_test, y_pred_test, A_test)\n",
    "        results.append(metrics)\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(os.path.join(args[\"output_dir\"], \"results.csv\"))\n",
    "    results.mean().to_csv(os.path.join(args[\"output_dir\"], \"results_mean.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(dataset_name):\n",
    "    experiments = glob.glob(f\"../results/group_experiment/{dataset_name}/*\")\n",
    "    results = []\n",
    "    for experiment in experiments:\n",
    "        df = pd.read_csv(os.path.join(experiment, \"results.csv\"))\n",
    "        df[\"experiment\"] = experiment.split(\"/\")[-1]\n",
    "        df[\"eq_loss\"] = 1 - df[\"eq_loss\"].abs()\n",
    "        df[\"spd\"] = 1 - df[\"spd\"].abs()\n",
    "        df[\"eod\"] = 1 - df[\"eod\"].abs()\n",
    "        results.append(df.iloc[:, 1:])\n",
    "    results = pd.concat(results)\n",
    "    # for each experiment, calculate the mean and std of each metric\n",
    "    results_mean = results.groupby(\"experiment\").mean()\n",
    "    results_std = results.groupby(\"experiment\").std()\n",
    "    \n",
    "    # combine dataframes into one with reorganized columns\n",
    "    results = pd.concat([results_mean, results_std], axis=1)\n",
    "    results.columns = pd.MultiIndex.from_product([[\"mean\", \"std\"], results_mean.columns])\n",
    "    results = results.swaplevel(axis=1)\n",
    "    results = results[[\"roc\", \"acc\", \"eod\",\"eq_loss\", \"spd\"]]\n",
    "    results = results.round(3)\n",
    "    print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:19<00:00, 43.90s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 1,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/XtremeFair_1\",\n",
    "    \"model_name\" : \"XtremeFair\",\n",
    "    \"n_trials\" : 100\n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:14<00:00, 67.43s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 1,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/XtremeFair_grad\",\n",
    "    \"model_name\" : \"XtremeFair_grad\",\n",
    "    \"n_trials\" : 100\n",
    "    \n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:20<00:00, 44.01s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 1,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/XGBClassifier\",\n",
    "    \"model_name\" : \"XGBClassifier\",\n",
    "    \"n_trials\" : 100\n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:36<00:00,  9.61s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 1,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/FairGBMClassifier\",\n",
    "    \"model_name\" : \"FairGBMClassifier\",\n",
    "    \"n_trials\" : 100\n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:45<01:42, 14.70s/it]/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:45: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:91: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:45: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:91: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      " 70%|███████   | 7/10 [01:51<00:47, 15.87s/it]/home/giovani/anaconda3/envs/dual_fair_boost/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/giovani/anaconda3/envs/dual_fair_boost/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 90%|█████████ | 9/10 [02:24<00:16, 16.61s/it]/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:45: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:37: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "/home/giovani/hiaac/dual_fair_boost/notebooks/../scripts/models.py:91: RuntimeWarning: overflow encountered in exp\n",
      "  predt = 1 / (1 + np.exp(-predt))\n",
      "100%|██████████| 10/10 [02:43<00:00, 16.31s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 0.75,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/XtremeFair_1_alpha_075\",\n",
    "    \"model_name\" : \"XtremeFair\",\n",
    "    \"n_trials\" : 100\n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:14<00:58, 19.49s/it]/home/giovani/anaconda3/envs/dual_fair_boost/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/giovani/anaconda3/envs/dual_fair_boost/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 10/10 [03:02<00:00, 18.28s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"dataset\" : \"german2\",\n",
    "    \"alpha\" : 0.75,\n",
    "    \"output_dir\" : \"../results/group_experiment/german2/XtremeFair_grad_alpha_075\",\n",
    "    \"model_name\" : \"XtremeFair_grad\",\n",
    "    \"n_trials\" : 100\n",
    "    \n",
    "}\n",
    "group_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     roc           acc           eod        eq_loss         \\\n",
      "                    mean    std   mean    std   mean    std    mean    std   \n",
      "experiment                                                                   \n",
      "FairGBMClassifier  0.769  0.049  0.705  0.050  0.846  0.123   0.889  0.107   \n",
      "XGBClassifier      0.784  0.045  0.705  0.067  0.890  0.072   0.909  0.090   \n",
      "XtremeFair_1       0.786  0.066  0.731  0.069  0.918  0.073   0.911  0.075   \n",
      "XtremeFair_grad    0.759  0.098  0.709  0.069  0.906  0.069   0.835  0.170   \n",
      "\n",
      "                     spd         \n",
      "                    mean    std  \n",
      "experiment                       \n",
      "FairGBMClassifier  0.856  0.113  \n",
      "XGBClassifier      0.860  0.078  \n",
      "XtremeFair_1       0.882  0.074  \n",
      "XtremeFair_grad    0.884  0.061  \n"
     ]
    }
   ],
   "source": [
    "summarize(\"german2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dual_fair_boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
